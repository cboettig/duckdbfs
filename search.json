[{"path":"https://cboettig.github.io/duckdbfs/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 duckdbfs authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://cboettig.github.io/duckdbfs/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Carl Boettiger. Author, maintainer.","code":""},{"path":"https://cboettig.github.io/duckdbfs/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Boettiger C (2023). duckdbfs: High Performance Remote File System Access Using 'duckdb'. https://github.com/cboettig/duckdbfs, https://cboettig.github.io/duckdbfs/.","code":"@Manual{,   title = {duckdbfs: High Performance Remote File System Access Using 'duckdb'},   author = {Carl Boettiger},   year = {2023},   note = {https://github.com/cboettig/duckdbfs, https://cboettig.github.io/duckdbfs/}, }"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"duckdbfs","dir":"","previous_headings":"","what":"High Performance Remote File System Access Using duckdb","title":"High Performance Remote File System Access Using duckdb","text":"duckdbfs simple wrapper around duckdb package facilitate working construction single lazy table (SQL connection) set file paths, URLs, S3 URIs.","code":""},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"High Performance Remote File System Access Using duckdb","text":"can install development version duckdbfs GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"cboettig/duckdbfs\")"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"quickstart","dir":"","previous_headings":"","what":"Quickstart","title":"High Performance Remote File System Access Using duckdb","text":"Imagine collection URLs files want combine single tibble R. files parquet csv, files may additional columns present files. combined data may large, potentially bigger available RAM slow download completely, may want subset using methods like dplyr::filter() dplyr::summarise(). can easily access data without downloading passing vector URLs. Note schemas (column names) match, must explicitly request duckdb join two schemas. Leave default, FALSE required achieve much better performance. Use filter(), select(), etc dplyr subset process data – method supported dbpylr. use dplyr::collect() trigger evaluation ingest results query R.","code":"library(duckdbfs) library(dplyr) #>  #> Attaching package: 'dplyr' #> The following objects are masked from 'package:stats': #>  #>     filter, lag #> The following objects are masked from 'package:base': #>  #>     intersect, setdiff, setequal, union base <- paste0(\"https://github.com/duckdb/duckdb/raw/main/\",                \"data/parquet-testing/hive-partitioning/union_by_name/\") f1 <- paste0(base, \"x=1/f1.parquet\") f2 <- paste0(base, \"x=1/f2.parquet\") f3 <- paste0(base, \"x=2/f2.parquet\") urls <- c(f1,f2,f3) ds <- open_dataset(urls, unify_schemas = TRUE) ds #> # Source:   table<kkkmtknecathhep> [3 x 4] #> # Database: DuckDB 0.8.1 [unknown@Linux 6.4.6-76060406-generic:R 4.3.1/:memory:] #>       i     j x         k #>   <int> <int> <chr> <int> #> 1    42    84 1        NA #> 2    42    84 1        NA #> 3    NA   128 2        33"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"s3-based-access","dir":"","previous_headings":"","what":"S3-based access","title":"High Performance Remote File System Access Using duckdb","text":"can also access remote data S3 protocol. advantage S3 unlike https, can discover files given folder, don’t list individually. particularly convenient accessing large, partitioned datasets, like GBIF: (nearly 200 GB data split across 2000 parquet files) additional configuration arguments passed helper function duckdb_s3_config() set access credentials configure settings, like alternative endpoints (use S3-compliant systems like minio). course also possible set ahead time calling duckdb_s3_config() directly. Many settings can also passed along compactly using URI query notation found arrow package. instance, can request anonymous access bucket alternative endpoint :","code":"parquet <- \"s3://gbif-open-data-us-east-1/occurrence/2023-06-01/occurrence.parquet\" duckdb_s3_config() gbif <- open_dataset(parquet, anonymous = TRUE, s3_region=\"us-east-1\") efi <- open_dataset(\"s3://anonymous@neon4cast-scores/parquet/aquatics?endpoint_override=data.ecoforecast.org\")"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"spatial-data","dir":"","previous_headings":"","what":"Spatial data","title":"High Performance Remote File System Access Using duckdb","text":"duckdb can also understand wide array spatial data queries spatial vector data, similar operations found popular sf package. spatial query operations require geometry column expresses simple feature geometry duckdb’s internal geometry format (nearly exactly WKB). common pattern first generate geometry column raw columns, latitude lognitude columns, using duckdb implementation method familiar postgis, ST_Point: Recall used sort external database like duckdb, dplyr functions like dplyr::mutate() transcribed SQL dbplyr, actually ever run R. allows us seamlessly pass along spatial functions like ST_Point, despite available R function. to_sf() coercion parse input SQL query gets passed duckdb, return object collected sf::st_read, returning (-memory) sf object. Note can add arbitrary spatial functions operate geometry, provided prior call to_sf. instance, first create geometry column lat/lon columns, compute distance element spatial point: details including complete list dozens spatial operations currently supported notes performance current limitations, see duckdb spatial docs","code":"spatial_ex <- paste0(\"https://raw.githubusercontent.com/cboettig/duckdbfs/\",                      \"main/inst/extdata/spatial-test.csv\") |>   open_dataset(format = \"csv\")   spatial_ex |>   mutate(geometry = ST_Point(longitude, latitude)) |>   to_sf() #> Simple feature collection with 10 features and 3 fields #> Geometry type: POINT #> Dimension:     XY #> Bounding box:  xmin: 1 ymin: 1 xmax: 10 ymax: 10 #> CRS:           NA #>    site latitude longitude      geometry #> 1     a        1         1   POINT (1 1) #> 2     b        2         2   POINT (2 2) #> 3     c        3         3   POINT (3 3) #> 4     d        4         4   POINT (4 4) #> 5     e        5         5   POINT (5 5) #> 6     f        6         6   POINT (6 6) #> 7     g        7         7   POINT (7 7) #> 8     h        8         8   POINT (8 8) #> 9     i        9         9   POINT (9 9) #> 10    j       10        10 POINT (10 10) spatial_ex |>    mutate(geometry = ST_Point(longitude, latitude)) |>   mutate(dist = ST_Distance(geometry, ST_Point(0,0))) |>    to_sf() #> Simple feature collection with 10 features and 4 fields #> Geometry type: POINT #> Dimension:     XY #> Bounding box:  xmin: 1 ymin: 1 xmax: 10 ymax: 10 #> CRS:           NA #>    site latitude longitude      dist      geometry #> 1     a        1         1  1.414214   POINT (1 1) #> 2     b        2         2  2.828427   POINT (2 2) #> 3     c        3         3  4.242641   POINT (3 3) #> 4     d        4         4  5.656854   POINT (4 4) #> 5     e        5         5  7.071068   POINT (5 5) #> 6     f        6         6  8.485281   POINT (6 6) #> 7     g        7         7  9.899495   POINT (7 7) #> 8     h        8         8 11.313708   POINT (8 8) #> 9     i        9         9 12.727922   POINT (9 9) #> 10    j       10        10 14.142136 POINT (10 10)"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"writing-datasets","dir":"","previous_headings":"","what":"Writing datasets","title":"High Performance Remote File System Access Using duckdb","text":"Like arrow::write_dataset(), duckdbfs::write_dataset() can write partitioned parquet files local disks also directly S3 bucket. Partitioned writes take advantage threading. Partition variables can specified explicitly, dplyr grouping variables used default:","code":"mtcars |> group_by(cyl, gear) |> write_dataset(tempfile())"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"local-files","dir":"","previous_headings":"","what":"Local files","title":"High Performance Remote File System Access Using duckdb","text":"course, open_dataset() write_dataset() also used local files. Remember parquet format required, can read csv files (including multiple hive-partitioned csv files).","code":"write.csv(mtcars, \"mtcars.csv\", row.names=FALSE) lazy_cars <- open_dataset(\"mtcars.csv\", format = \"csv\")"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"limitations","dir":"","previous_headings":"","what":"Limitations","title":"High Performance Remote File System Access Using duckdb","text":"NOTE: time, duckdb httpfs file system extension R support Windows.","code":""},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"mechanism--motivation","dir":"","previous_headings":"","what":"Mechanism / motivation","title":"High Performance Remote File System Access Using duckdb","text":"package simply creates duckdb connection, ensures httpfs spatial extensions installed necessary, sets S3 configuration, constructs VIEW using duckdb’s parquet_scan() read_csv_auto() methods associated options. returns dplyr::tbl() resulting view. Though straightforward, process substantially verbose analogous single function call provided arrow::open_dataset() due mostly necessary string manipulation construct VIEW SQL statement. ’ve used pattern lot, especially arrow option (http data) substantially worse performance (many S3 URIs).","code":""},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"advanced-notes","dir":"","previous_headings":"","what":"Advanced notes","title":"High Performance Remote File System Access Using duckdb","text":"similar behavior arrow::open_dataset(), exceptions: time, arrow support access HTTP – remote sources must S3 GC-based object store. local file system S3 paths, duckdb can support “globbing” point path, e.g. open_dataset(data/*/subdir). (Like arrow, duckdbfs::open_dataset assume recursive path discovery directories). Note http(s) URLs always require full vector since ls() method possible. Even URLs vector-based paths, duckdb can automatically populate column names given hive structure hive_style=TRUE (default). Note passing vector paths can significantly faster globbing S3 sources ls() operation relatively expensive many partitions.","code":""},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"performance-notes","dir":"","previous_headings":"","what":"Performance notes","title":"High Performance Remote File System Access Using duckdb","text":"settings, duckdbfs::open_dataset can give substantially better performance (orders magnitude) arrow::open_dataset(), settings may comparable even slower. Package versions, system libraries, network architecture, remote storage performance, network traffic, factors can influence performance, making precise benchmark comparisons real-world contexts difficult. slow network connections accessing remote table repeatedly, may improve performance create local copy table rather perform operations network. simplest way setting mode = \"TABLE\" instead “VIEW” open dataset. probably desirable pass duckdb connection backed persistent disk location case instead default cached_connection() unless available RAM limiting. unify_schema computationally expensive. Ensuring files/partitions match schema advance processing different files separately can greatly improve performance.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"create a cachable duckdb connection — cached_connection","title":"create a cachable duckdb connection — cached_connection","text":"function primarily intended internal use duckdbfs functions.  However, can called directly user whenever desirable direct access connection object.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"create a cachable duckdb connection — cached_connection","text":"","code":"cached_connection(dbdir = \":memory:\", read_only = FALSE)"},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"create a cachable duckdb connection — cached_connection","text":"dbdir Location database files. path existing directory file system. default, data kept RAM read_only Set TRUE read-operation","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"create a cachable duckdb connection — cached_connection","text":"duckdb::duckdb() connection object","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"create a cachable duckdb connection — cached_connection","text":"first called (user internal function), function creates duckdb connection places connection cache (duckdbfs_conn option). subsequent calls, function returns cached connection, rather recreating fresh connection. frees user responsibility managing connection object, functions needing access connection can use create access existing connection. close global environment, function's finalizer gracefully shutdown connection removing cache.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"create a cachable duckdb connection — cached_connection","text":"","code":"con <- cached_connection() close_connection(con)"},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"close connection — close_connection","title":"close connection — close_connection","text":"close connection","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"close connection — close_connection","text":"","code":"close_connection(conn = cached_connection())"},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"close connection — close_connection","text":"conn duckdb connection (leave blank) Closes invisible cached connection duckdb","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"close connection — close_connection","text":"returns nothing.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"close connection — close_connection","text":"Shuts connection gc removes . clear cached reference avoid using stale connection avoids complaint connection garbage collected.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"close connection — close_connection","text":"","code":"close_connection()"},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Configure S3 settings for database connection — duckdb_s3_config","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"function used configure S3 settings database connection. allows set various S3-related parameters access key, secret access key, endpoint, region, session token, uploader settings, URL compatibility mode, URL style, SSL usage.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"","code":"duckdb_s3_config(   conn = cached_connection(),   s3_access_key_id = NULL,   s3_secret_access_key = NULL,   s3_endpoint = NULL,   s3_region = NULL,   s3_session_token = NULL,   s3_uploader_max_filesize = NULL,   s3_uploader_max_parts_per_file = NULL,   s3_uploader_thread_limit = NULL,   s3_url_compatibility_mode = NULL,   s3_url_style = NULL,   s3_use_ssl = NULL,   anonymous = NULL )"},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"conn database connection object created using cache_connection function (default: cache_connection()). s3_access_key_id S3 access key ID (default: NULL). s3_secret_access_key S3 secret access key (default: NULL). s3_endpoint S3 endpoint (default: NULL). s3_region S3 region (default: NULL). s3_session_token S3 session token (default: NULL). s3_uploader_max_filesize maximum filesize S3 uploader (50GB 5TB, default 800GB). s3_uploader_max_parts_per_file maximum number parts per file S3 uploader (1 10000, default 10000). s3_uploader_thread_limit thread limit S3 uploader (default: 50). s3_url_compatibility_mode Disable Globs Query Parameters S3 URLs (default: 0, allows globs/queries). s3_url_style style S3 URLs use. Default \"vhost\" unless s3_endpoint set, makes default \"path\" (.e. MINIO systems). s3_use_ssl Enable disable SSL S3 connections (default: 1 (TRUE)). anonymous request anonymous access (sets s3_access_key_id s3_secret_access_key \"\", allowing anonymous access public buckets).","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"Returns silently (NULL) successful.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"see https://duckdb.org/docs/sql/configuration.html","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"","code":"if (FALSE) { # interactive() # Configure S3 settings duckdb_s3_config(            s3_access_key_id = \"YOUR_ACCESS_KEY_ID\",            s3_secret_access_key = \"YOUR_SECRET_ACCESS_KEY\",            s3_endpoint = \"YOUR_S3_ENDPOINT\",            s3_region = \"YOUR_S3_REGION\",            s3_uploader_max_filesize = \"800GB\",            s3_uploader_max_parts_per_file = 100,            s3_uploader_thread_limit = 8,            s3_url_compatibility_mode = FALSE,            s3_url_style = \"vhost\",            s3_use_ssl = TRUE,            anonymous = TRUE) }"},{"path":"https://cboettig.github.io/duckdbfs/reference/load_spatial.html","id":null,"dir":"Reference","previous_headings":"","what":"load the duckdb geospatial data plugin — load_spatial","title":"load the duckdb geospatial data plugin — load_spatial","text":"load duckdb geospatial data plugin","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/load_spatial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"load the duckdb geospatial data plugin — load_spatial","text":"","code":"load_spatial(conn = cached_connection())"},{"path":"https://cboettig.github.io/duckdbfs/reference/load_spatial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"load the duckdb geospatial data plugin — load_spatial","text":"conn database connection object created using cache_connection function (default: cache_connection()).","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/load_spatial.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"load the duckdb geospatial data plugin — load_spatial","text":"loads extension returns status invisibly.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/load_spatial.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"load the duckdb geospatial data plugin — load_spatial","text":"https://duckdb.org/docs/extensions/spatial.html","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a dataset from a variety of sources — open_dataset","title":"Open a dataset from a variety of sources — open_dataset","text":"function opens dataset variety sources, including Parquet, CSV, etc, using either local file system paths, URLs, S3 bucket URI notation.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a dataset from a variety of sources — open_dataset","text":"","code":"open_dataset(   sources,   schema = NULL,   hive_style = TRUE,   unify_schemas = FALSE,   format = c(\"parquet\", \"csv\", \"tsv\", \"text\"),   conn = cached_connection(),   tblname = tmp_tbl_name(),   mode = \"VIEW\",   filename = FALSE,   recursive = TRUE,   ... )"},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a dataset from a variety of sources — open_dataset","text":"sources character vector paths dataset files. schema schema dataset. NULL, schema inferred dataset files. hive_style logical value indicating whether dataset uses Hive-style partitioning. unify_schemas logical value indicating whether unify schemas dataset files (union_by_name). TRUE, execute UNION column name across files (NOTE: can add considerably initial execution time) format format dataset files. One \"parquet\", \"csv\", \"tsv\", \"text\". conn connection database. tblname name table create database. mode mode create table . One \"VIEW\" \"TABLE\". Creating VIEW, default, execute quickly create local copy dataset.  TABLE create local copy duckdb's native format, downloading full dataset necessary. using TABLE mode large data, please sure use conn connections disk-based storage, e.g. calling cached_connection(), e.g. cached_connection(\"storage_path\"), otherwise full data must fit RAM.  Using TABLE assumes familiarity R's DBI-based interface. filename logical value indicating whether include filename table name. recursive assume recursive path? default TRUE. Set FALSE trying open single, un-partitioned file. ... optional additional arguments passed duckdb_s3_config(). Note apply set URI notation thus may used override provide settings supported format.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open a dataset from a variety of sources — open_dataset","text":"lazy dplyr::tbl object representing opened dataset backed duckdb SQL connection.  dplyr (tidyr) verbs can used directly object, can translated SQL commands automatically via dbplyr.  Generic R commands require using dplyr::collect() table, forces evaluation reading resulting data memory.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open a dataset from a variety of sources — open_dataset","text":"","code":"if (FALSE) { # interactive() # A remote, hive-partitioned Parquet dataset base <- paste0(\"https://github.com/duckdb/duckdb/raw/main/\",              \"data/parquet-testing/hive-partitioning/union_by_name/\") f1 <- paste0(base, \"x=1/f1.parquet\") f2 <- paste0(base, \"x=1/f2.parquet\") f3 <- paste0(base, \"x=2/f2.parquet\")  open_dataset(c(f1,f2,f3), unify_schemas = TRUE)  # Access an S3 database specifying an independently-hosted (MINIO) endpoint efi <- open_dataset(\"s3://neon4cast-scores/parquet/aquatics\",                     s3_access_key_id=\"\",                     s3_endpoint=\"data.ecoforecast.org\") }"},{"path":"https://cboettig.github.io/duckdbfs/reference/to_sf.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert output to sf object — to_sf","title":"Convert output to sf object — to_sf","text":"Convert output sf object","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/to_sf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert output to sf object — to_sf","text":"","code":"to_sf(x, conn = cached_connection())"},{"path":"https://cboettig.github.io/duckdbfs/reference/to_sf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert output to sf object — to_sf","text":"x remote duckdb tbl (open_dataset) dplyr-pipeline thereof. conn connection object tbl. Takes duckdb table (open_dataset) dataset dplyr pipline returns sf object. Important: table must geometry column, almost always create first. Note: to_sf() triggers collection R.  function suitable use end dplyr pipeline subset data. Using function large dataset without filtering first may exceed available memory.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/to_sf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert output to sf object — to_sf","text":"sf class object (memory).","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/to_sf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert output to sf object — to_sf","text":"","code":"if (FALSE) { # interactive()  library(dplyr) csv_file <- system.file(\"extdata/spatial-test.csv\", package=\"duckdbfs\")  # Note that we almost always must first create a `geometry` column, e.g. # from lat/long columns using the `st_point` method. sf <-   open_dataset(csv_file, format = \"csv\") |>   mutate(geometry = ST_Point(longitude, latitude)) |>   to_sf()  # We can use the full space of spatial operations, including spatial # and normal dplyr filters.  All operations are translated into a # spatial SQL query by `to_sf`: open_dataset(csv_file, format = \"csv\") |>   mutate(geometry = ST_Point(longitude, latitude)) |>   mutate(dist = ST_Distance(geometry, ST_Point(0,0))) |>   filter(site %in% c(\"a\", \"b\", \"e\")) |>   to_sf()  }"},{"path":"https://cboettig.github.io/duckdbfs/reference/write_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"write_dataset — write_dataset","title":"write_dataset — write_dataset","text":"write_dataset","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/write_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"write_dataset — write_dataset","text":"","code":"write_dataset(   dataset,   path,   conn = cached_connection(),   format = c(\"parquet\", \"csv\"),   partitioning = dplyr::group_vars(dataset),   overwrite = TRUE,   ... )"},{"path":"https://cboettig.github.io/duckdbfs/reference/write_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"write_dataset — write_dataset","text":"dataset remote tbl object open_dataset, -memory data.frame. path local file path S3 path write credentials conn duckdbfs database connection format export format partitioning names columns use partition variables overwrite allow overwriting existing files? ... additional arguments duckdb_s3_config()","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/write_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"write_dataset — write_dataset","text":"Returns path, invisibly.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/write_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"write_dataset — write_dataset","text":"","code":"if (FALSE) { # interactive()   write_dataset(mtcars, tempfile()) } if (FALSE) { # interactive() write_dataset(mtcars, tempdir()) }"},{"path":"https://cboettig.github.io/duckdbfs/news/index.html","id":"duckdbfs-002","dir":"Changelog","previous_headings":"","what":"duckdbfs 0.0.2","title":"duckdbfs 0.0.2","text":"duckdbfs now spatial data query support! Users can leverage spatial data operations like st_distance() st_area() request return values sf objects. Supports network-based access . See README.md Added write_dataset() can write (potentially partitioned) parquet local directories remote (S3) buckets. S3 interface supports arrow-compatible URI notation: Alternate endpoints can now passed like s3://userid:secret_token@bucket-name?endpoint_override=data.ecoforecast.org Users can omit use * (match file) ** (recursive search) just supply path. Recursive search assumed automatically. Note: unlike arrow, still supports use globs (*) elsewhere path, e.g. s3://bucket/*/path duckdb_s3_config gains argument anonymous allowing users ignore existing AWS keys may set environmental variables AWS configuration files. can also passed username position URI notation, e.g. s3://anonymous@bucket_name. open_dataset drops use endpoint argument. Instead, alternative S3 endpoints can set either using URI query notation calling duckdb_s3_config() first. Additionally, arguments duckdb_s3_config(), including s3_endpoint, can now passed open_dataset .... Note settings override set URI notation.","code":""},{"path":"https://cboettig.github.io/duckdbfs/news/index.html","id":"duckdbfs-001","dir":"Changelog","previous_headings":"","what":"duckdbfs 0.0.1","title":"duckdbfs 0.0.1","text":"CRAN release: 2023-08-09 Initial release CRAN","code":""}]
