[{"path":"https://cboettig.github.io/duckdbfs/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 duckdbfs authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://cboettig.github.io/duckdbfs/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Carl Boettiger. Author, maintainer.","code":""},{"path":"https://cboettig.github.io/duckdbfs/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Boettiger C (2023). duckdbfs: High Performance Remote File System Access Using 'duckdb'. https://github.com/cboettig/duckdbfs, https://cboettig.github.io/duckdbfs/.","code":"@Manual{,   title = {duckdbfs: High Performance Remote File System Access Using 'duckdb'},   author = {Carl Boettiger},   year = {2023},   note = {https://github.com/cboettig/duckdbfs, https://cboettig.github.io/duckdbfs/}, }"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"duckdbfs","dir":"","previous_headings":"","what":"High Performance Remote File System Access Using duckdb","title":"High Performance Remote File System Access Using duckdb","text":"duckdbfs simple wrapper around duckdb package facilitate working construction single lazy table (SQL connection) set file paths, URLs, S3 URIs.","code":""},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"High Performance Remote File System Access Using duckdb","text":"can install development version duckdbfs GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"cboettig/duckdbfs\")"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"High Performance Remote File System Access Using duckdb","text":"Imagine collection URLs files want combine single tibble R. files parquet csv, files may additional columns present files. combined data may large, potentially bigger available RAM slow download completely, may want subset using methods like dplyr::filter() dplyr::summarise(). can easily access data without downloading passing vector URLs. Note schemas (column names) match, must explicitly request duckdb join two schemas. Leave default, FALSE required achieve much better performance. Use filter(), select(), etc dplyr subset process data – method supported dbpylr. use dplyr::collect() trigger evaluation ingest results query R.","code":"base <- paste0(\"https://github.com/duckdb/duckdb/raw/master/\",                \"data/parquet-testing/hive-partitioning/union_by_name/\") f1 <- paste0(base, \"x=1/f1.parquet\") f2 <- paste0(base, \"x=1/f2.parquet\") f3 <- paste0(base, \"x=2/f2.parquet\") urls <- c(f1,f2,f3) library(duckdbfs)  ds <- open_dataset(urls, unify_schemas = TRUE) ds #> # Source:   table<nfzdryqkqgxdgbf> [3 x 4] #> # Database: DuckDB 0.8.1 [unknown@Linux 5.17.15-76051715-generic:R 4.3.1/:memory:] #>       i     j x         k #>   <int> <int> <chr> <int> #> 1    42    84 1        NA #> 2    42    84 1        NA #> 3    NA   128 2        33"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"s3-based-access","dir":"","previous_headings":"","what":"S3-based access","title":"High Performance Remote File System Access Using duckdb","text":"can also access remote data S3 protocol. advantage S3 unlike https, can use wildcard globbing file system can list files. S3 system can also support write access. Use duckdb_s3_config() function set access credentials configure settings, like alternative endpoints (use S3-compliant systems like minio).","code":"parquet <- \"s3://gbif-open-data-us-east-1/occurrence/2023-06-01/*/*\" gbif <- open_dataset(parquet)"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"local-files","dir":"","previous_headings":"","what":"Local files","title":"High Performance Remote File System Access Using duckdb","text":"course, open_dataset() can also used local files. Remember parquet format required, can read csv files (including multiple hive-partitioned csv files).","code":"write.csv(mtcars, \"mtcars.csv\", row.names=FALSE) lazy_cars <- open_dataset(\"mtcars.csv\", format = \"csv\")"},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"mechanism--motivation","dir":"","previous_headings":"","what":"Mechanism / motivation","title":"High Performance Remote File System Access Using duckdb","text":"package simply creates duckdb connection, ensures httpfs extension installed necessary, constructs VIEW using duckdb’s parquet_scan() read_csv_auto() methods associated options. returns dplyr::tbl() resulting view. Though straightforward, process substantially verbose analogous single function call provided arrow::open_dataset() due mostly necessary string manipulation construct VIEW SQL statement. ’ve used pattern lot, especially arrow option (http data) substantially worse performance (many S3 URIs).","code":""},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"advanced-notes","dir":"","previous_headings":"","what":"Advanced notes","title":"High Performance Remote File System Access Using duckdb","text":"similar behaviour arrow::open_dataset(), exceptions: time, arrow support access HTTP – remote sources must S3 GC-based object store. local file system S3 paths, duckdb can support “globbing” recursive globbing, e.g. open_dataset(data/**/*.parquet). contrast, http(s) URLs always require full vector since ls() method possible. However, note even URLs, duckdb can automatically populate columns given hive structure. Also note passing vector paths can significantly faster globbing S3 sources ls() operation relatively expensive. NOTE: time, duckdb httpfs file system extension R support Windows.","code":""},{"path":"https://cboettig.github.io/duckdbfs/index.html","id":"performance-notes","dir":"","previous_headings":"","what":"Performance notes","title":"High Performance Remote File System Access Using duckdb","text":"slow network connections accessing remote table repeatedly, may improve performance create local copy table rather perform operations network. simplest way setting mode = \"TABLE\" instead “VIEW” open dataset. probably desirable pass duckdb connection backed persistent disk location case instead default cached_connection() unless available RAM limiting.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"create a cachable duckdb connection — cached_connection","title":"create a cachable duckdb connection — cached_connection","text":"function primarily intended internal use duckdbfs functions.  However, can called directly user whenever desirable direct access connection object.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"create a cachable duckdb connection — cached_connection","text":"","code":"cached_connection(dbdir = \":memory:\", read_only = FALSE)"},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"create a cachable duckdb connection — cached_connection","text":"dbdir Location database files. path existing directory file system. default, data kept RAM read_only Set TRUE read-operation","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"create a cachable duckdb connection — cached_connection","text":"first called (user internal function), function creates duckdb connection places connection cache (duckdbfs_conn option). subsequent calls, function returns cached connection, rather recreating fresh connection. frees user responsibility managing connection object, functions needing access connection can use create access existing connection. close global environment, function's finalizer gracefully shutdown connection removing cache.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/cached_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"create a cachable duckdb connection — cached_connection","text":"","code":"con <- cached_connection() close_connection(con)"},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"close connection — close_connection","title":"close connection — close_connection","text":"close connection","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"close connection — close_connection","text":"","code":"close_connection(conn = cached_connection())"},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"close connection — close_connection","text":"conn duckdb connection (leave blank) Closes invisible cached connection duckdb","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"close connection — close_connection","text":"Shuts connection gc removes . clear cached reference avoid using stale connection avoids complaint connection garbage collected.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/close_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"close connection — close_connection","text":"","code":"close_connection()"},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Configure S3 settings for database connection — duckdb_s3_config","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"function used configure S3 settings database connection. allows set various S3-related parameters access key, secret access key, endpoint, region, session token, uploader settings, URL compatibility mode, URL style, SSL usage.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"","code":"duckdb_s3_config(   conn = cached_connection(),   s3_access_key_id = NULL,   s3_secret_access_key = NULL,   s3_endpoint = NULL,   s3_region = NULL,   s3_session_token = NULL,   s3_uploader_max_filesize = NULL,   s3_uploader_max_parts_per_file = NULL,   s3_uploader_thread_limit = NULL,   s3_url_compatibility_mode = NULL,   s3_url_style = NULL,   s3_use_ssl = NULL )"},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"conn database connection object created using cache_connection function (default: cache_connection()). s3_access_key_id S3 access key ID (default: NULL). s3_secret_access_key S3 secret access key (default: NULL). s3_endpoint S3 endpoint (default: NULL). s3_region S3 region (default: NULL). s3_session_token S3 session token (default: NULL). s3_uploader_max_filesize maximum filesize S3 uploader (50GB 5TB, default 800GB). s3_uploader_max_parts_per_file maximum number parts per file S3 uploader (1 10000, default 10000). s3_uploader_thread_limit thread limit S3 uploader (default: 50). s3_url_compatibility_mode Disable Globs Query Parameters S3 URLs (default: 0, allows globs/queries). s3_url_style style S3 URLs use. Default \"vhost\" unless s3_endpoint set, makes default \"path\" (.e. MINIO systems). s3_use_ssl Enable disable SSL S3 connections (default: 1 (TRUE)).","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"see https://duckdb.org/docs/sql/configuration.html","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/duckdb_s3_config.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Configure S3 settings for database connection — duckdb_s3_config","text":"","code":"if (FALSE) { # interactive() # Configure S3 settings duckdb_s3_config(            s3_access_key_id = \"YOUR_ACCESS_KEY_ID\",            s3_secret_access_key = \"YOUR_SECRET_ACCESS_KEY\",            s3_endpoint = \"YOUR_S3_ENDPOINT\",            s3_region = \"YOUR_S3_REGION\",            s3_uploader_max_filesize = \"800GB\",            s3_uploader_max_parts_per_file = 100,            s3_uploader_thread_limit = 8,            s3_url_compatibility_mode = FALSE,            s3_url_style = \"vhost\",            s3_use_ssl = TRUE) }"},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a dataset from a variety of sources — open_dataset","title":"Open a dataset from a variety of sources — open_dataset","text":"function opens dataset variety sources, including Parquet, CSV, etc, using either local file system paths, URLs, S3 bucket URI notation.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a dataset from a variety of sources — open_dataset","text":"","code":"open_dataset(   sources,   schema = NULL,   hive_style = TRUE,   unify_schemas = FALSE,   format = c(\"parquet\", \"csv\", \"tsv\", \"text\"),   conn = cached_connection(),   tblname = tmp_tbl_name(),   mode = \"VIEW\",   filename = FALSE,   endpoint = NULL )"},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a dataset from a variety of sources — open_dataset","text":"sources character vector paths dataset files. schema schema dataset. NULL, schema inferred dataset files. hive_style logical value indicating whether dataset uses Hive-style partitioning. unify_schemas logical value indicating whether unify schemas dataset files (union_by_name). TRUE, execute UNION column name across files (NOTE: can add considerably initial execution time) format format dataset files. One \"parquet\", \"csv\", \"tsv\", \"text\". conn connection database. tblname name table create database. mode mode create table . One \"VIEW\" \"TABLE\". filename logical value indicating whether include filename table name. endpoint optionally, alternative endpoint S3 object store.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open a dataset from a variety of sources — open_dataset","text":"lazy dplyr::tbl object representing opened dataset backed duckdb SQL connection.  dplyr (tidyr) verbs can used directly object, can translated SQL commands automatically via dbplyr.  Generic R commands require using dplyr::collect() table, forces evaluation reading resulting data memory.","code":""},{"path":"https://cboettig.github.io/duckdbfs/reference/open_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open a dataset from a variety of sources — open_dataset","text":"","code":"if (FALSE) { # duckdbfs:::example_safe() # Open a remote, hive-partitioned Parquet dataset base <- paste0(\"https://github.com/duckdb/duckdb/raw/master/\",              \"data/parquet-testing/hive-partitioning/union_by_name/\") f1 <- paste0(base, \"x=1/f1.parquet\") f2 <- paste0(base, \"x=1/f2.parquet\") f3 <- paste0(base, \"x=2/f2.parquet\")  open_dataset(c(f1,f2,f3)) }"}]
